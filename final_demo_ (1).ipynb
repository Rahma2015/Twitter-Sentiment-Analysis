{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final demo  .ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzE79cIj1fL1"
      },
      "source": [
        "# Importing liberaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2tdCzxo1fL4"
      },
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMFMD20ztBUW"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJM8OxPQ1fL4",
        "outputId": "532355f7-3597-4149-d9f3-acb1246ef39b"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow4Y9WnK1fL5",
        "outputId": "a632ea9d-0dbd-47f0-a63c-b56c68f917cd"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjjudeTd1fL6"
      },
      "source": [
        "# Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A9K1jIo-7Ow"
      },
      "source": [
        "data= pd.read_csv('training.1600000.processed.noemoticon.csv',  encoding='ISO-8859-1', names=['1','2','3','4','5', '6'], header =None )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "clCDXM5tAseB",
        "outputId": "48f0e61d-d6db-4457-cd0f-50b81b48e7f5"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   1  ...                                                  6\n",
              "0  0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1  0  ...  is upset that he can't update his Facebook by ...\n",
              "2  0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3  0  ...    my whole body feels itchy and like its on fire \n",
              "4  0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk1bed1x1fL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4669f698-a9bd-498f-cb23-923dced08807"
      },
      "source": [
        "#number of records in the dataset\n",
        "print('The number of records in the dataset is '+ str(len(data))+ ' record.') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of records in the dataset is 1600000 record.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay4R1q0A1fL7",
        "scrolled": true,
        "outputId": "2741de9d-54c1-4ea1-eff5-f2521d9b1622"
      },
      "source": [
        "#checking for null values\n",
        "data.isnull().sum().sum() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQOUgcOJ1fL8"
      },
      "source": [
        "#change the target and tweets columns' names \n",
        "data = data.rename(columns = {'1': 'targ', '6': 'texts'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ6WptxyE1Nk",
        "outputId": "cab6f936-9ae6-4eeb-e974-0325326a324b"
      },
      "source": [
        "data['targ'][data['targ']==4]=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcUD-wF2DZhL"
      },
      "source": [
        "data_pos = data[data['targ'] == 1]\n",
        "data_neg = data[data['targ'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s65ftUboDedE"
      },
      "source": [
        "data_pos = data_pos.iloc[:int(50000)]\n",
        "data_neg = data_neg.iloc[:int(50000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTvuA1ySDhWY"
      },
      "source": [
        "data = pd.concat([data_pos, data_neg])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "hiZK25gAEY6Y",
        "outputId": "31ad1017-ee36-45a5-ebcf-09f2070e4eba"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targ</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>800000</th>\n",
              "      <td>1</td>\n",
              "      <td>1467822272</td>\n",
              "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ersle</td>\n",
              "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800001</th>\n",
              "      <td>1</td>\n",
              "      <td>1467822273</td>\n",
              "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>becca210</td>\n",
              "      <td>im meeting up with one of my besties tonight! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800002</th>\n",
              "      <td>1</td>\n",
              "      <td>1467822283</td>\n",
              "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Wingman29</td>\n",
              "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800003</th>\n",
              "      <td>1</td>\n",
              "      <td>1467822287</td>\n",
              "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>katarinka</td>\n",
              "      <td>Being sick can be really cheap when it hurts t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800004</th>\n",
              "      <td>1</td>\n",
              "      <td>1467822293</td>\n",
              "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_EmilyYoung</td>\n",
              "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>0</td>\n",
              "      <td>1678337109</td>\n",
              "      <td>Sat May 02 06:22:31 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Jessica_567</td>\n",
              "      <td>@mileycyrus so i have the same insomnia prob a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>0</td>\n",
              "      <td>1678337116</td>\n",
              "      <td>Sat May 02 06:22:31 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>whouwit077</td>\n",
              "      <td>20 mintues late for my meeting starting @ 8  h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>0</td>\n",
              "      <td>1678337128</td>\n",
              "      <td>Sat May 02 06:22:31 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmyPR</td>\n",
              "      <td>@kentucky_derby super excited! Are you tweetin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>0</td>\n",
              "      <td>1678337159</td>\n",
              "      <td>Sat May 02 06:22:32 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>cynthia_sue03</td>\n",
              "      <td>I WANT ANOTHER DAY OFF!!!!  To much Sh#t to do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>0</td>\n",
              "      <td>1678337793</td>\n",
              "      <td>Sat May 02 06:22:40 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>msceo8</td>\n",
              "      <td>i just jacked up this umbrella cake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        targ  ...                                              texts\n",
              "800000     1  ...       I LOVE @Health4UandPets u guys r the best!! \n",
              "800001     1  ...  im meeting up with one of my besties tonight! ...\n",
              "800002     1  ...  @DaRealSunisaKim Thanks for the Twitter add, S...\n",
              "800003     1  ...  Being sick can be really cheap when it hurts t...\n",
              "800004     1  ...    @LovesBrooklyn2 he has that effect on everyone \n",
              "...      ...  ...                                                ...\n",
              "49995      0  ...  @mileycyrus so i have the same insomnia prob a...\n",
              "49996      0  ...  20 mintues late for my meeting starting @ 8  h...\n",
              "49997      0  ...  @kentucky_derby super excited! Are you tweetin...\n",
              "49998      0  ...  I WANT ANOTHER DAY OFF!!!!  To much Sh#t to do...\n",
              "49999      0  ...               i just jacked up this umbrella cake \n",
              "\n",
              "[100000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "FvwPeAYc1fL8",
        "outputId": "3c8e8383-42cd-4ad1-8461-a76e94ed3e9f"
      },
      "source": [
        "data_nw = data.drop(columns = ['2','3','4','5'])\n",
        "\n",
        "data_nw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targ</th>\n",
              "      <th>texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>800000</th>\n",
              "      <td>1</td>\n",
              "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800001</th>\n",
              "      <td>1</td>\n",
              "      <td>im meeting up with one of my besties tonight! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800002</th>\n",
              "      <td>1</td>\n",
              "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800003</th>\n",
              "      <td>1</td>\n",
              "      <td>Being sick can be really cheap when it hurts t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800004</th>\n",
              "      <td>1</td>\n",
              "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        targ                                              texts\n",
              "800000     1       I LOVE @Health4UandPets u guys r the best!! \n",
              "800001     1  im meeting up with one of my besties tonight! ...\n",
              "800002     1  @DaRealSunisaKim Thanks for the Twitter add, S...\n",
              "800003     1  Being sick can be really cheap when it hurts t...\n",
              "800004     1    @LovesBrooklyn2 he has that effect on everyone "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTKYDaw01fL9"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGNwwcZE1fL9",
        "outputId": "ed712151-f7a7-4bf0-80a8-b8e76ab9e143"
      },
      "source": [
        "#stop words list \n",
        "list_stp_wrds = stopwords.words('english')\n",
        "list_stp_wrds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hDRjg1X1fL9",
        "outputId": "32bf77f3-919d-4516-a129-057ae7ae7381"
      },
      "source": [
        "for stpwrd in list_stp_wrds:\n",
        "    if stpwrd.endswith(\"n't\") | stpwrd.endswith(\"dn\") | stpwrd.endswith(\"sn\"):\n",
        "        list_stp_wrds.remove(stpwrd)\n",
        "list_stp_wrds      #updated  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " 'couldn',\n",
              " 'didn',\n",
              " 'doesn',\n",
              " 'hadn',\n",
              " 'hasn',\n",
              " 'haven',\n",
              " 'isn',\n",
              " 'ma',\n",
              " 'mightn',\n",
              " 'mustn',\n",
              " 'needn',\n",
              " 'shan',\n",
              " 'shouldn',\n",
              " 'wasn',\n",
              " 'weren',\n",
              " 'won',\n",
              " 'wouldn']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEZzYZkg-jJh"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI8-9u4__JxI",
        "outputId": "f175b813-088c-4d1d-b83c-f9bc27beb57a"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3o2WSRU1fL-"
      },
      "source": [
        "def Preprocessing(tw):\n",
        "    \n",
        "    emoji = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
        "              ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "              ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "              ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "              '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "              '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
        "              ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "\n",
        "    links = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    mentions = r\"@[^\\s]+\"\n",
        "    nonalpha = '[^a-zA-Z0-9]'\n",
        "    \n",
        "    \n",
        "    \n",
        "    tw = re.sub(links,'',tw)\n",
        "    \n",
        "    tw = re.sub(mentions, '', tw) \n",
        "    \n",
        "    \n",
        "    \n",
        "    for em in emoji.keys():\n",
        "        tw = tw.replace(em, emoji[em])   \n",
        "    \n",
        "    tw = re.sub(nonalpha, \" \", tw)     \n",
        "    \n",
        "    tokens = word_tokenize(tw)   \n",
        "    lemm = '';\n",
        "    \n",
        "    for wrd in tokens:\n",
        "        if wrd not in list_stp_wrds:  \n",
        "            if len(wrd) > 1:\n",
        "                obj = WordNetLemmatizer()\n",
        "                  \n",
        "\n",
        "                lem_wrd = obj.lemmatize(wrd)  \n",
        "                lemm += (lem_wrd + ' ')    \n",
        "    \n",
        "    return lemm.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yosw8p-e1fL-",
        "scrolled": true,
        "outputId": "94e8db03-9b5c-4803-9309-becc64f9e0ee"
      },
      "source": [
        "data_nw['texts'] = data_nw['texts'].apply(Preprocessing)\n",
        "data_nw['texts']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000                                        LOVE guy best\n",
              "800001    im meeting one besties tonight Cant wait GIRL ...\n",
              "800002    Thanks Twitter add Sunisa got meet HIN show DC...\n",
              "800003    Being sick really cheap hurt much eat real foo...\n",
              "800004                                      effect everyone\n",
              "                                ...                        \n",
              "49995     insomnia prob slept hr woke 5am nd couldnt go ...\n",
              "49996     20 mintues late meeting starting know going la...\n",
              "49997     super excited Are tweeting event happening Onl...\n",
              "49998     WANT ANOTHER DAY OFF To much Sh today Got quot...\n",
              "49999                                  jacked umbrella cake\n",
              "Name: texts, Length: 100000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxjLqbveAbVH"
      },
      "source": [
        "X=data.texts\n",
        "y=data.targ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VejHqF1VBfU9"
      },
      "source": [
        "#Preparing the input features for training \n",
        "\n",
        "We converting the text words into arrays form.\n",
        "Maximum 500 features/words selected for training. These 500 words will be selected on the importance that will distinguish between the positive tweets and negative tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLMfRxntA0Gm"
      },
      "source": [
        "max_len = 500\n",
        "tok = Tokenizer(num_words=1000)\n",
        "tok.fit_on_texts(X)\n",
        "sequences = tok.texts_to_sequences(X)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAPDrIIF_6Ql",
        "outputId": "93f726ff-5026-4e90-98c2-0be65d00b876"
      },
      "source": [
        "sequences_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo5RcHPBBpVA"
      },
      "source": [
        "total 100000 tweets and the number words/features are 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT70E6ruBzP9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V8iwFSxB44z"
      },
      "source": [
        "Separating the 70% data for training data and 30% for testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ljZ8KPfBqRi"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsU62WcfB6HT"
      },
      "source": [
        "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
        "    inputs = Input(name='inputs',shape=[max_len])#step1\n",
        "    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n",
        "    layer = LSTM(64)(layer) #step3\n",
        "    layer = Dense(256,name='FC1')(layer) #step4\n",
        "    layer = Activation('relu')(layer) # step5\n",
        "    layer = Dropout(0.5)(layer) # step6\n",
        "    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n",
        "    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n",
        "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
        "    return model #function returning the value when we call it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgxbIvLJCC83"
      },
      "source": [
        "model = tensorflow_based_model() # here we are calling the function of created model\n",
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6PK8DEyCTht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f410063-279c-42ac-bcfe-5c030b28a607"
      },
      "source": [
        "history=model.fit(X_train,Y_train,batch_size=100,epochs=10, validation_split=0.1)# here we are starting the training of model by feeding the training data\n",
        "print('Training finished !!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "630/630 [==============================] - 18s 27ms/step - loss: 0.5277 - accuracy: 0.7357 - val_loss: 0.4859 - val_accuracy: 0.7594\n",
            "Epoch 2/10\n",
            "630/630 [==============================] - 17s 26ms/step - loss: 0.4898 - accuracy: 0.7630 - val_loss: 0.4825 - val_accuracy: 0.7656\n",
            "Epoch 3/10\n",
            "630/630 [==============================] - 16s 26ms/step - loss: 0.4819 - accuracy: 0.7686 - val_loss: 0.4740 - val_accuracy: 0.7674\n",
            "Epoch 4/10\n",
            "630/630 [==============================] - 17s 27ms/step - loss: 0.4648 - accuracy: 0.7779 - val_loss: 0.4696 - val_accuracy: 0.7699\n",
            "Epoch 5/10\n",
            "630/630 [==============================] - 16s 26ms/step - loss: 0.4609 - accuracy: 0.7822 - val_loss: 0.4681 - val_accuracy: 0.7710\n",
            "Epoch 6/10\n",
            "630/630 [==============================] - 16s 26ms/step - loss: 0.4506 - accuracy: 0.7869 - val_loss: 0.4673 - val_accuracy: 0.7714\n",
            "Epoch 7/10\n",
            "630/630 [==============================] - 17s 27ms/step - loss: 0.4461 - accuracy: 0.7882 - val_loss: 0.5024 - val_accuracy: 0.7503\n",
            "Epoch 8/10\n",
            "630/630 [==============================] - 17s 26ms/step - loss: 0.4506 - accuracy: 0.7862 - val_loss: 0.4769 - val_accuracy: 0.7719\n",
            "Epoch 9/10\n",
            "630/630 [==============================] - 17s 26ms/step - loss: 0.4359 - accuracy: 0.7943 - val_loss: 0.4680 - val_accuracy: 0.7767\n",
            "Epoch 10/10\n",
            "630/630 [==============================] - 16s 26ms/step - loss: 0.4311 - accuracy: 0.7966 - val_loss: 0.4680 - val_accuracy: 0.7724\n",
            "Training finished !!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdLPut8GDGM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1409380b-d4ff-419d-8bb0-b5da3a7482ab"
      },
      "source": [
        "accr1 = model.evaluate(X_test,Y_test) #we are starting to test the model here\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 9s 9ms/step - loss: 0.4856 - accuracy: 0.7668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tHT19KFDG9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c20be5-9f07-4128-96c6-ed2c4d03a0bd"
      },
      "source": [
        "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set\n",
            "  Accuracy: 0.77\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}